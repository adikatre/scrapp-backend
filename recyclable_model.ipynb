{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30af952a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3574d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a3f93",
   "metadata": {},
   "source": [
    "# Augmentations, Preprocessing and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44821b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12000 images belonging to 30 classes.\n",
      "Found 3000 images belonging to 30 classes.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/RecyclableAndHouseholdWasteClassification/images'\n",
    "\n",
    "image_size = (256, 256) # also try training using 256x256 images\n",
    "batch_size = 32 # change this to 16, 32, 64, or 128 and compare the results\n",
    "seed = 1337 # for repeatability in train/validation split\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0, # normalize pixels\n",
    "    validation_split=0.2, # 80/20 percent split\n",
    "    rotation_range = 20, # rotate images 20 degrees CW or CCW\n",
    "    width_shift_range = 0.1, # shift images up or down 10 %\n",
    "    height_shift_range = 0.1, # shift images right or left 10%\n",
    "    shear_range = 0.1, #shears (distorts) images\n",
    "    zoom_range = 0.15, # zoom in or out\n",
    "    horizontal_flip = True,\n",
    "    brightness_range = [0.8, 0.12], # mimics real-world brightness inconsistency \n",
    "    fill_mode = 'nearest' # fill in new pixels during augumentations \n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    validation_split = 0.2\n",
    "    \n",
    ")\n",
    "\n",
    "# load train/valid data\n",
    "\n",
    "train_data = datagen.flow_from_directory(data_dir, target_size=image_size, batch_size=batch_size, class_mode='categorical', subset='training', shuffle = True, seed = seed)\n",
    "\n",
    "valid_data = datagen.flow_from_directory(data_dir, target_size=image_size, batch_size=batch_size, class_mode='categorical', subset='validation', shuffle = True, seed = seed)\n",
    "\n",
    "num_classes = len(train_data.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea245e2",
   "metadata": {},
   "source": [
    "# Build the Model off a EfficientNetB0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB0(\n",
    "    include_top = False, \n",
    "    weights = 'imagenet',\n",
    "    input_shape=(image_size[0], image_size[1], 3) #keep size always consistent\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = layers.Input(shape=(image_size[0], image_size[1], 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x) # drop 40% of neurons; avoid overfitting\n",
    "\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = models.Model(inputs=inputs, outputs=outputs, name=\"efficientnetb0_transfer\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed58c41",
   "metadata": {},
   "source": [
    "# Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b2bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "913a19bd",
   "metadata": {},
   "source": [
    "# Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6564000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 451ms/step - accuracy: 0.1508 - loss: 4.4899 - val_accuracy: 0.4187 - val_loss: 2.1075\n",
      "Epoch 2/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 487ms/step - accuracy: 0.6150 - loss: 1.4932 - val_accuracy: 0.6103 - val_loss: 1.5791\n",
      "Epoch 3/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 506ms/step - accuracy: 0.8770 - loss: 0.5025 - val_accuracy: 0.6420 - val_loss: 1.6903\n",
      "Epoch 4/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 510ms/step - accuracy: 0.9441 - loss: 0.2489 - val_accuracy: 0.6633 - val_loss: 1.5205\n",
      "Epoch 5/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 464ms/step - accuracy: 0.9607 - loss: 0.1618 - val_accuracy: 0.6763 - val_loss: 1.5118\n",
      "Epoch 6/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 483ms/step - accuracy: 0.9659 - loss: 0.1275 - val_accuracy: 0.6813 - val_loss: 1.5385\n",
      "Epoch 7/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 365ms/step - accuracy: 0.9617 - loss: 0.1234 - val_accuracy: 0.6727 - val_loss: 1.4568\n",
      "Epoch 8/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 382ms/step - accuracy: 0.9668 - loss: 0.0997 - val_accuracy: 0.6730 - val_loss: 1.5650\n",
      "Epoch 9/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 496ms/step - accuracy: 0.9683 - loss: 0.0836 - val_accuracy: 0.6727 - val_loss: 1.5554\n",
      "Epoch 10/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 506ms/step - accuracy: 0.9696 - loss: 0.0759 - val_accuracy: 0.6560 - val_loss: 1.7114\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,  # play with this number to maximize accuracy\n",
    "    validation_data=valid_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f85e5",
   "metadata": {},
   "source": [
    "# Validation for accuracy\n",
    "Note: check `validation_split=0.05` in \"Preprocessing and Preparation.\" Use `validation_split=0.2` to get an accuracy score that better reflects the model's performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b785fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 109ms/step - accuracy: 0.6541 - loss: 1.6623\n",
      "Validation Loss: 1.7113932371139526, Validation Accuracy: 0.656000018119812\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 104ms/step - accuracy: 0.9707 - loss: 0.1036\n",
      "Training Loss: 0.09901190549135208, Training Accuracy: 0.9704166650772095\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(valid_data)\n",
    "print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_acc}')\n",
    "\n",
    "train_loss, train_acc = model.evaluate(train_data)\n",
    "print(f'Training Loss: {train_loss}, Training Accuracy: {train_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db677a",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae00c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309abb78",
   "metadata": {},
   "source": [
    "# Use the Model to Make a Prediction for a user uploaded Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44fcba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted class: plastic_food_containers\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "uploaded_img_path = 'test_imgs/img4.jpeg'  # Replace with the actual path of the uploaded image\n",
    "uploaded_img = load_img(uploaded_img_path, target_size=image_size)\n",
    "\n",
    "# Convert the image to an array and normalize it\n",
    "uploaded_img_array = img_to_array(uploaded_img) / 255.0\n",
    "uploaded_img_array = np.expand_dims(uploaded_img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "# Make a prediction\n",
    "uploaded_prediction = model.predict(uploaded_img_array)\n",
    "\n",
    "# Decode the prediction\n",
    "predicted_class = np.argmax(uploaded_prediction, axis=1)\n",
    "class_labels = {v: k for k, v in train_data.class_indices.items()}  # Reverse the class indices dictionary\n",
    "predicted_label = class_labels[predicted_class[0]]\n",
    "\n",
    "print(f'Predicted class: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf39de72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.class_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
